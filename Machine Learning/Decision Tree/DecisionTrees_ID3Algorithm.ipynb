{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aaronmat1905/MLdiaries/blob/main/DecisionTrees_ID3Algorithm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHm_55Sb_KgK"
      },
      "source": [
        "# Desicion Trees **(August 2025)**\n",
        "## **ID3 Algorithm**\n",
        "The ID3 (Iterative Dichotomiser 3) algorithm is a classic algoritm used to build Decision Trees in Machine Learning.\n",
        "\n",
        "\n",
        "\n",
        "*Introduced by Ross Quinlan in 1986*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbO4ZoCu_ol6"
      },
      "source": [
        "How does ID3 Work? \n",
        "\n",
        "**High Level Overview**\n",
        "\n",
        "ID3 builds a decision tree by\n",
        "1. Selecting the **best attribute** to split data at each step\n",
        "2. **Recursively partitioning** the dataset into subsets based on that attribute\n",
        "3. Continuing untill all data points in a subset belong to the same class (**No attributes remain**)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxODjToFAFdP"
      },
      "source": [
        "**Steps**\n",
        "1. Calculate the **Entropy** of the DataSet.\n",
        "2. For each attribute, calculate the **Information Gain** if we split on it\n",
        "3. *Select* the attribute with the **maximum Information Gain** as the decision node\n",
        "4. **Create Child nodes** for each possible value of the chosen attribute\n",
        "5. ***Repeat*** the process recursively for each  subset until:\n",
        "- All Samples in a subset are of the same class\n",
        "- No Attributes remain -> Use Majority class\n",
        "- Subset is empty -> also assign majority class of the parent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9g9z0khIzuQq"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwDNA0aWA3Pi"
      },
      "source": [
        "## **Entropy**\n",
        "Entropy is a measure of impurity or uncertainity in a dataset.\n",
        "- If a dataset has all samples from a single class, entropy is 0\n",
        "- If a dataset has an Even mix of classes, entropy is **maximum**, where most uncertainity is prevalent\n",
        "\n",
        "For categorical data, it tell you how diverse the categories are:\\\n",
        " *Are there different varietes of categories? if yes, then there might be maximum uncertainity in predicting a category.*\n",
        "\n",
        "\n",
        "It's Formula is given by:\n",
        "\n",
        "$$\n",
        "H(S) = - \\sum_{i=1}^{c} p_i \\log_2 p_i\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "**H(S)**: Entropy of Data Set\n",
        "\n",
        "**𝑪**: Number of unique categories/clases\n",
        "\n",
        "**𝓅_i**: is the Proportion of Samples belonging to class *(The probability of class i in dataset S)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeGJNKCADfQm"
      },
      "source": [
        "**For a Dataset, How do you calculate the Entropy?**\n",
        "1. Compute Class Probabilities\n",
        "- Find the probability of each class in the dataset.\n",
        "$$\n",
        "P(X) = {No. of times X occurs}/{No. of datapoints}\n",
        "$$\n",
        "2. Plug it into the formula"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wGolczvh0s_x",
        "outputId": "6e9c8484-359d-4867-b55c-f96fdb126bc1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unique Values: ['0' '1' 'no' 'yes']\tData Type: <class 'numpy.ndarray'>\n",
            "Frequencies of Unique Values: [3 3 2 1]\tData Type: <class 'numpy.ndarray'>\n",
            "Probabilities: [0.33333333 0.33333333 0.22222222 0.11111111]\tData Type: <class 'numpy.ndarray'>\n",
            "\n",
            "\n",
            "Entropy of the Demo Dataset: 1.8910611120726526\n"
          ]
        }
      ],
      "source": [
        "# Dry run of Entropy:\n",
        "data = np.array([\n",
        "    [1, 1, \"no\"],\n",
        "    [0, 0, \"yes\"],\n",
        "    [1, 0, \"no\"]\n",
        "])\n",
        "\n",
        "# Getting Count of the unique elements:\n",
        "values, count = np.unique(data, return_counts = True)\n",
        "print(f\"Unique Values: {values}\\tData Type: {type(values)}\")\n",
        "print(f\"Frequencies of Unique Values: {count}\\tData Type: {type(count)}\")\n",
        "\n",
        "# Calculating the Probabilities/fraction of all the unique elements present\n",
        "probabilities = count/count.sum()\n",
        "print(f\"Probabilities: {probabilities}\\tData Type: {type(count)}\")\n",
        "\n",
        "# Entropy:\n",
        "entropy_demo = - np.sum(probabilities* np.log2(probabilities))\n",
        "print(f\"\\n\\nEntropy of the Demo Dataset: {entropy_demo}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_mt7e5MHxir"
      },
      "source": [
        "**Interpreting the value of an Entropy**\\\n",
        "A Value of 1.891 signifies that the Dataset has high impurity. This is because we can see 4 unique classes within the dataset, and the distribution is uneven and not spread out.\n",
        "An Entropy value of >1 is normal when we have multiple classes.\n",
        "\n",
        "**Notes**\n",
        "- If all samples were the same class: Entropy = 0\n",
        "- If all samples are perfectly distributed about 4 classes: Entropy ~ 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "QKJSg8f8HwsT"
      },
      "outputs": [],
      "source": [
        "# Defining Function:\n",
        "def entropy(data):\n",
        "  \"Calculates the Entropy | Input: np.array | Output: float\"\n",
        "  values, count = np.unique(data, return_counts = True)\n",
        "  probabilities = count/count.sum()\n",
        "  return - np.sum(probabilities*np.log2(probabilities))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-8i-pAaJWUN"
      },
      "source": [
        "## **Average Information**: Expected Entropy after split on attribute A\n",
        "The **Expected Entropy** is a measure of how impure the dataset remains after splitting on that attribute to divide the data into subsets.\n",
        "\n",
        "*The average information of an attribute is the weighted sum of entropies of the subsets created by splitting the dataset on that attribute, representing the expected impurity after the split.*\n",
        "\n",
        "***\"How much uncertainity remains about the target variable after we know the value of the attribute A\"*** [When we split on attribute A]\n",
        "\n",
        "\n",
        "___\n",
        "Formally,\n",
        "$$\n",
        "Info_A(S) = \\sum_{v \\in Values(A)} \\frac{|S_v|}{|S|} \\cdot H(S_v)\n",
        "$$\n",
        "Where:\n",
        "- 𝐒 is the full dataset\n",
        "- 𝚨 is the attribute we are evaluating\n",
        "- 𝐒𝔳 is the subset of 𝐒 where 𝚨 has value 𝔳\n",
        "- |𝐒𝔳|/|𝐒 | is the weight (proportion of samples with the value 𝔳"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvKTcoFxK2ui"
      },
      "source": [
        "When we split the dataset using attribute 𝚨, each branch/subset has some kind of impurity left. Due to the differing size of the subsets, we take a weighted average of their entropies. **Hence, this value tells us the expected disorder if we split on attribute 𝚨**\\\n",
        "\n",
        "**Why It Matters?**\n",
        "- If an attribute produces **pure subsets** → Average information is low → Lesser Impurity.\n",
        "- If it produces **mixed subset** → Average information is high → More Impurity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tR8_M8PmL3Gp",
        "outputId": "26612249-e1c7-4517-d489-e4b72da10d8d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "np.float64(0.6666666666666666)"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## Average Information: Dry Run\n",
        "attribute = 1 # Column index of the attribute whose average information we want\n",
        "total_samples = len(data)\n",
        "\n",
        "# Finding the Unique values of the attributes and their frequencies\n",
        "vals, counts = np.unique(data[:, attribute], return_counts=True)\n",
        "avg_info = 0.0\n",
        "\n",
        "# Iterate through each unique value of the attribute\n",
        "for v, count in zip(vals, counts):\n",
        "  subset = data[data[:,attribute]==v]\n",
        "  subset_entropy = entropy(subset[:, -1])\n",
        "  avg_info += (count/total_samples)*subset_entropy\n",
        "\n",
        "avg_info"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SvTqxy4JNCtS"
      },
      "source": [
        "What does these lines do?\n",
        "- `subset` gets only the rows, where attribute = 𝔳\n",
        "- `subset[:, -1]` takes the last column: Target colum\n",
        "- It then calculates entropy of that target's distribution\n",
        "- **Weighting**: Multiply that entropy by the fraciton `count\\total_samples`\n",
        "- **Accumulate**: Sum these weighted entropies into `avg_info`\n",
        "\n",
        "\n",
        "*In essence, even after splitting, the subsets are not perfectly pure: there is still some confusion about class labels. Here I've taken a very very small data set*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "7eiOG8OJN6h4"
      },
      "outputs": [],
      "source": [
        "def average_information(data, attribute):\n",
        "  values, count = np.unique(data[:, attribute], return_counts=True)\n",
        "  total_samples = len(data)\n",
        "  avg_info = 0.0\n",
        "  for v, count in zip(values, count):\n",
        "    subset = data[data[:, attribute]==v]\n",
        "    subset_entropy = entropy(subset)\n",
        "    avg_info += (count/total_samples)* subset_entropy\n",
        "  return avg_info"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWUGgVNMOdMK"
      },
      "source": [
        "## **Information Gain**\n",
        "\n",
        "Information Gain measures how much an attribute reduces uncertainity about the target variable.\n",
        "\n",
        "$$\n",
        "IG(S, A) = H(S) - \\sum_{v \\in \\text{Values}(A)} \\frac{|S_v|}{|S|} \\cdot H(S_v)\n",
        "$$\n",
        "\n",
        "\n",
        "**Difference between Information Gain and Average Information**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "FgWuXptF_V_S"
      },
      "outputs": [],
      "source": [
        "def information_gain(data, attribute):\n",
        "  dataset_entropy = entropy(data)\n",
        "  avg_info = average_information(data, attribute)\n",
        "  return round(dataset_entropy-avg_info, 4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O72roKWN_xz4"
      },
      "outputs": [],
      "source": [
        "def get_selected_attribute(data):\n",
        "  n_attributes = data.shape[1]-1\n",
        "  gains = {}\n",
        "  for attr in range(n_attributes):\n",
        "    gains[attr] = information_gain(data, attr)\n",
        "  best_attr = max(gains, key = gains.get)\n",
        "  return gains, best_attr"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPIziI+bnVM0swyZzpQUcn5",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
