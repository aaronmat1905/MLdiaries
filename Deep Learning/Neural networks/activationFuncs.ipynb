{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf0d8022",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "import numpy as np\n",
    "\n",
    "# Declaring \"e\" for future use\n",
    "e = math.e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96202da",
   "metadata": {},
   "source": [
    "**Activation Functions**\n",
    "\n",
    "> Are **mathematical functions** used within a neural network that determines whether the Neuron is fired or not.\n",
    "\n",
    "*Formal Definition*\n",
    "> Are Non-linear Transformations applied to the next layer of the neurons\n",
    "$$\n",
    "Y = \\text{Activation}\\!\\left(\\sum_{i=1}^{n} W_i X_i \\right)\n",
    "$$\n",
    "\n",
    "**Properties of Activation Functions?**\n",
    "- Non-Linear\n",
    "    - Non-Linearity enables the model to understand complex patterns from the data\n",
    "- Differentiable\n",
    "    - Activation functions should be differentiable in order to facilitate the *Back-propogation* process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e09896",
   "metadata": {},
   "source": [
    "## Linear Function \n",
    "$$\n",
    "f(x) = x \n",
    "$$ \n",
    "\n",
    "- It could be used for the output layer for regression \n",
    "- It recognizes only linear shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f788655",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(x): \n",
    "    return x "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff453423",
   "metadata": {},
   "source": [
    "## Sigmoid\n",
    "$$\n",
    "f(x) = \\frac{1} {1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "The Output value is mapped between 0 and 1. \n",
    "However, this function introduces *Vanishing Gradient* \n",
    "\n",
    "> Vanishing Gradient is the phenomenon where the weights of a NN approximate itself to zero/negligible hence causing training resources to be spent. It does not change to weight updates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e91ad712",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x): \n",
    "    return 1/(1 + (e**-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95405047",
   "metadata": {},
   "source": [
    "## TanH\n",
    "$$\n",
    "f(x) = \\frac{e^x - e^{-x}} {e^x + e^{-x}}\n",
    "$$ \n",
    "\n",
    "The Output value is mapped between -1 and 1. \n",
    "\n",
    "**How is this different from Sigmoid?** \n",
    "- Here, smaller-lesser values are spread around 0, hence improved training effect and weight adjustments move in the right direction \n",
    "- Scales smaller values in the output range\n",
    "\n",
    "However, both Sigmoid and TanH suffer from the **Vanishing Gradient** and **Saturation Effect**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb1559b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanH(x): \n",
    "    return (e**x - e**(-x))/ (e**x + e**(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29cdf349",
   "metadata": {},
   "source": [
    "## ReLU: **Rectified Linear Unit**\n",
    "\n",
    "$$\n",
    "f(x) = max(0, x)\n",
    "$$\n",
    "\n",
    "> Definition in function itself.\n",
    "\n",
    "*Advantages*: \n",
    "- Simple Calculation\n",
    "- No Vanishing Gradient \n",
    "- Better results for newer model architectures\n",
    "- More economic value \n",
    "\n",
    "*Disadvantages*: \n",
    "- 20-50% of neurons die off \n",
    "- Highly dependent on a well-choosen learning rate\n",
    "- Theoretically, it can assume large values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d9edcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x): \n",
    "    return max(0, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b969cfa",
   "metadata": {},
   "source": [
    "## Leaky ReLU \n",
    "\n",
    "$$\n",
    "f(x) = max(ax, x)\n",
    "$$ \n",
    "\n",
    "Here, even if the neuron recieves negative values, it still does not become zero and can generate a small gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15ba44be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaky_relu(a, x): \n",
    "    return max(a*x, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c9247e",
   "metadata": {},
   "source": [
    "## **Softmax**\n",
    "\n",
    "$$\n",
    "\\sigma_j = \\frac{e^{z_j}}{\\sum_{k=1}^{K} e^{z_k}}\n",
    "$$\n",
    "> Takes a vector, convert's it's values as probabilities depending on their size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32ef785b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softMax(x: np.ndarray):\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
