{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aaronmat1905/MLdiaries/blob/main/Artificial%20Neural%20Networks/ArtificialNeuralNetworks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0RhI9kUQZcIs"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPQ0CZGeaNz6"
      },
      "source": [
        "**Assume that:**\n",
        "- **Polynomial Type**:\n",
        "$$\n",
        "y = 0.96x^2 + 7.35x + 7.07\n",
        "$$\n",
        "- **Noise Level**:\n",
        "$$\n",
        "œµ \\sim N(0, 2.42)\n",
        "$$\n",
        "- **Architecture**: \\\n",
        "Input(1) ‚Üí Hidden(32) ‚Üí Hidden(72) ‚Üí Output(1)\n",
        "- **Learning Rate (Œ±)** = 0.005\n",
        "- **Architecture**: Narrow-To-Wide Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-mMBs4WDrlR"
      },
      "source": [
        "# **Data Loading**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "id": "hdpedx_xcGYM",
        "outputId": "3b61c889-e6fa-4a9e-f985-1bfd3851a597"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 100000 entries, 0 to 99999\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count   Dtype  \n",
            "---  ------  --------------   -----  \n",
            " 0   x       100000 non-null  float64\n",
            " 1   y       100000 non-null  float64\n",
            "dtypes: float64(2)\n",
            "memory usage: 1.5 MB\n",
            "\n",
            "+++++++++++++++++++++++++\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"ann_data\",\n  \"rows\": 100000,\n  \"fields\": [\n    {\n      \"column\": \"x\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 57.65028013334275,\n        \"min\": -99.99975935929236,\n        \"max\": 99.99902373611826,\n        \"num_unique_values\": 100000,\n        \"samples\": [\n          -75.83968803332172,\n          41.16539782811199,\n          10.830659449117915\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"y\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2874.4460450064653,\n        \"min\": -15.039608966161218,\n        \"max\": 10301.60993226984,\n        \"num_unique_values\": 100000,\n        \"samples\": [\n          4941.731489137534,\n          1924.995765800459,\n          199.72377837945643\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "ann_data"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-60a7a9b3-1d06-4629-b474-e4cfb81a75a3\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>x</th>\n",
              "      <th>y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-55.601366</td>\n",
              "      <td>2547.648326</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>74.146461</td>\n",
              "      <td>5803.114349</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-58.656169</td>\n",
              "      <td>2864.092846</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>83.722182</td>\n",
              "      <td>7324.122723</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-2.317762</td>\n",
              "      <td>-1.245122</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-60a7a9b3-1d06-4629-b474-e4cfb81a75a3')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-60a7a9b3-1d06-4629-b474-e4cfb81a75a3 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-60a7a9b3-1d06-4629-b474-e4cfb81a75a3');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-80c8282f-2c21-4006-b995-ad5a9792859b\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-80c8282f-2c21-4006-b995-ad5a9792859b')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-80c8282f-2c21-4006-b995-ad5a9792859b button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "           x            y\n",
              "0 -55.601366  2547.648326\n",
              "1  74.146461  5803.114349\n",
              "2 -58.656169  2864.092846\n",
              "3  83.722182  7324.122723\n",
              "4  -2.317762    -1.245122"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Loading in our dataset: <Synthetically Generated>\n",
        "# Loading in our dataset: <Synthetically Generated>\n",
        "datasetPath = \"ann_dataset.csv\"\n",
        "ann_data = pd.read_csv(datasetPath)\n",
        "ann_data.info()\n",
        "print(\"\\n+++++++++++++++++++++++++\\n\")\n",
        "ann_data.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HkVC0RgAhSpO"
      },
      "source": [
        "# **Activation Functions**\n",
        "**ReLU**:\n",
        "\n",
        "*Rectified Linear Unit*\n",
        "$$\n",
        "f(x) = max(0, x)\n",
        "$$\n",
        "- The ReLU function returns the maximum between between it's input and zero.\n",
        "- Advantages:\n",
        "  - It helps mitigate the Vanishing gradient problem.\n",
        "  - Leads to efficient computation, since, It eliminates all negative outputs.\n",
        "  - Allows NNs to scale to many layers without a significant increase in computational burden.\n",
        "\n",
        "___\n",
        "**ReLU Derivative**\n",
        "The derivative of a function measures how much it changes it's slope:\n",
        "\n",
        "*Here, we have to measure it's slope/derivative of the function in each region*\n",
        "\n",
        "**Case Analysis:**\n",
        "- Case 1: ùí≥ > 0:\n",
        "  - In this region, f(ùí≥) = ùí≥ ; **f'(ùí≥) = 1**\n",
        "- Case 2: ùí≥ < 0:\n",
        "  - In this region, f(ùí≥) = 0 ; **f'(ùí≥) = 0**\n",
        "- Case 3: ùí≥ = 0:\n",
        "  - Left Hand Derivative: = 0\n",
        "  - Right Hand Derivative: = 1\n",
        "  - Therefore, the derivative at ùí≥ = 0 is undefined.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "eFBhdhPLhSQT"
      },
      "outputs": [],
      "source": [
        "def relu(z):\n",
        "  return max(0, z)\n",
        "\n",
        "def relu_derivative(z):\n",
        "  answer = relu(z)\n",
        "  if answer < 0:\n",
        "    return 0\n",
        "  elif answer > 0:\n",
        "    return 1\n",
        "  else:\n",
        "    return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFhgpkuDpjqV"
      },
      "source": [
        "# **Loss Function**\n",
        "**MSE**:\n",
        "\n",
        "MSE, or Mean Squared error measures the average squared difference between the actual observed values and values predicted by a model.\n",
        "- Smaller MSE ‚áí Models predictions closer to the actual data.\n",
        "\n",
        "$$\n",
        "\\mathcal{L}_{\\text{MSE}} = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "mE9AOWxoprmV"
      },
      "outputs": [],
      "source": [
        "def MSE(y_true, y_pred):\n",
        "  # Assuming y_true and y_pred are numpy arrays\n",
        "  N = len(y_true)\n",
        "  return (1/N)*np.sum(y_true - y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWSOsL1r7PIR"
      },
      "source": [
        "# **Weight Initialization**\n",
        "> Weight initialization is the process of setting the starting values of the weights (parameters) in a neural network before the training begins.\n",
        "\n",
        "Since training relies on *gradient descent*, the initial weights heavily influence the:\n",
        "  - Convergence speed (How fast training progresses)\n",
        "  - Avoiding poor local minima\n",
        "  - Preventing vanishing or exploding gradients in deep network.\n",
        "\n",
        "**Why not start with all 0s?**\n",
        "\n",
        "If so, every neuron in a layer learns the same thing (*symmetry problem*). Gradient updates will be identical, so network won't learn effectively.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgE1nSOg8o2O"
      },
      "source": [
        "---\n",
        "\n",
        "## **Xavier (Glorot) Initialization**\n",
        "\n",
        "### ***The Core Idea***\n",
        "\n",
        "A neuron‚Äôs output is a weighted sum of its inputs:\n",
        "\n",
        "$$\n",
        "z_j^{(l)} = \\sum_{i=1}^{n_{in}} W_{ji}^{(l)} x_i^{(l-1)}\n",
        "$$\n",
        "\n",
        "Here:\n",
        "\n",
        "* $n_{in}$ = number of inputs (fan-in)\n",
        "* $n_{out}$ = number of outputs (fan-out)\n",
        "* $W_{ji}$ = weights\n",
        "* $x^{(l-1)}$ = activations from the previous layer\n",
        "\n",
        "For stable training, we want two things:\n",
        "\n",
        "1. The **variance of activations** remains consistent across layers (forward stability).\n",
        "2. The **variance of gradients** remains consistent across layers (backward stability).\n",
        "\n",
        "---\n",
        "\n",
        "### ***Forward Stability***\n",
        "\n",
        "The variance of the output $z_j^{(l)}$ depends on the variance of weights and inputs:\n",
        "\n",
        "$$\n",
        "Var[z_j^{(l)}] = n_{in} \\cdot Var[W] \\cdot Var[x^{(l-1)}]\n",
        "$$\n",
        "\n",
        "To avoid activations shrinking or exploding, we want:\n",
        "\n",
        "$$\n",
        "Var[z_j^{(l)}] \\approx Var[x^{(l-1)}]\n",
        "$$\n",
        "\n",
        "This gives:\n",
        "\n",
        "$$\n",
        "Var[W] = \\frac{1}{n_{in}}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### ***Backward Stability***\n",
        "\n",
        "During backpropagation, gradients flow through weights as well. Their variance depends on $n_{out}$:\n",
        "\n",
        "$$\n",
        "Var\\left[\\frac{\\partial L}{\\partial x^{(l-1)}}\\right] = n_{out} \\cdot Var[W] \\cdot Var\\left[\\frac{\\partial L}{\\partial z^{(l)}}\\right]\n",
        "$$\n",
        "\n",
        "For stable gradients:\n",
        "\n",
        "$$\n",
        "Var[W] = \\frac{1}{n_{out}}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### ***The Compromise***\n",
        "\n",
        "* Forward pass prefers: $Var[W] = \\frac{1}{n_{in}}$\n",
        "* Backward pass prefers: $Var[W] = \\frac{1}{n_{out}}$\n",
        "\n",
        "Xavier/Glorot initialization takes a **compromise** between the two:\n",
        "\n",
        "$$\n",
        "Var[W] = \\frac{2}{n_{in} + n_{out}}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### ***Practical Formulations***\n",
        "\n",
        "From this variance, we can define two initialization schemes:\n",
        "\n",
        "* **Uniform Distribution**\n",
        "\n",
        "$$\n",
        "W \\sim U\\left(-\\sqrt{\\frac{6}{n_{in}+n_{out}}}, \\; \\sqrt{\\frac{6}{n_{in}+n_{out}}}\\right)\n",
        "$$\n",
        "\n",
        "* **Normal Distribution**\n",
        "\n",
        "$$\n",
        "W \\sim \\mathcal{N}\\left(0, \\frac{2}{n_{in}+n_{out}}\\right)\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "‚úÖ **Summary:**\n",
        "Xavier Initialization is a weight initialization method that ensures both forward activations and backward gradients maintain a stable variance throughout the network, preventing vanishing or exploding signals. It achieves this by balancing between the input and output layer sizes, setting the weight variance to:\n",
        "\n",
        "$$\n",
        "Var[W] = \\frac{2}{n_{in} + n_{out}}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7ywHRS-_pws"
      },
      "source": [
        "Pseudocode for the Xavier-init method:\n",
        "```\n",
        "function Xavier_Init(n_in, n_out, distribution):\n",
        "    if distribution == \"uniform\":\n",
        "        limit = sqrt(6 / (n_in + n_out))\n",
        "        W = random_uniform(-limit, +limit, size=(n_out, n_in))\n",
        "    else if distribution == \"normal\":\n",
        "        sigma = sqrt(2 / (n_in + n_out))\n",
        "        W = random_normal(0, sigma, size=(n_out, n_in))\n",
        "    return W\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "pvv-gH-s9yHd"
      },
      "outputs": [],
      "source": [
        "def xavier_initialization(data, input_dim, hidden1, hidden2, output_dim):\n",
        "  seed = data[1][0]\n",
        "  np.random.seed(seed)\n",
        "\n",
        "  # Input -> Hidden1\n",
        "  xavier_std1 = np.sqrt(2/(input_dim+hidden1))\n",
        "  w1 = np.random.normal(0, xavier_std1, (hidden1, input_dim)) # Initialize weights\n",
        "  b1 = np.zeroes((hidden1, 1)) # Initializing Biases\n",
        "\n",
        "  # Hidden1 -> Hidden2\n",
        "  xavier_std2 = np.sqrt(2/(hidden1+hidden2))\n",
        "  w2 = np.random.normal(0, xavier_std2, (hidden2, hidden1))\n",
        "  b2 = np.zeroes((hidden2, 1))\n",
        "\n",
        "  # Hidden2 -> Output\n",
        "  xavier_std3 = np.sqrt(2/(hidden2+output_dim))\n",
        "  w3 = np.random.normal(0, xavier_std3, (output_dim, hidden2))\n",
        "  b3 = np.zeroes((output_dim, 1))\n",
        "\n",
        "  return w1, b1, w2, b2, w3, b3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WlWKyRxDbQ3"
      },
      "source": [
        "# **Forward Propogation**\n",
        "Forward propogation is the process by which **input data is passed through a neural network to compute output predictions**.\n",
        "\n",
        "It involves performing linear transformations followed by non-linear activations at each layer.\n",
        "\n",
        "Consider a feedforward neural network with:\n",
        "\n",
        "* Input layer: $X \\in \\mathbb{R}^{n \\times d_{\\text{in}}}$\n",
        "* Hidden layers: with weights $W^{[l]}$ and biases $b^{[l]}$\n",
        "* Output layer: with weights $W^{[L]}$ and biases $b^{[L]}$\n",
        "\n",
        "**The Steps are:**\n",
        "1. Linear Transformation (Pre-activation)\n",
        "\n",
        "For the $l$-th layer:\n",
        "\n",
        "$$\n",
        "z^{[l]} = a^{[l-1]} W^{[l]} + b^{[l]}\n",
        "$$\n",
        "\n",
        "* $a^{[l-1]}$ is the activation from the previous layer (or input $X$ for the first layer).\n",
        "* $W^{[l]}$ are the layer weights, $b^{[l]}$ are biases.\n",
        "* $z^{[l]}$ is called the **pre-activation** of layer $l$.\n",
        "\n",
        "2. Activation Function:\n",
        "\n",
        "After computing the pre-activation, $$a^{[l]} = g(z^{[l]})$$\n",
        "\n",
        "The activation function introduces non-linear capabilities, allowing the network to model complex relationships\n",
        "\n",
        "3. Output Layer:\n",
        "\n",
        "For Regression Tasks, the output layer is often linear.\n",
        "$$y^{^}=z[L]$$\n",
        "\n",
        "For classification, a softmax or sigmoid activation is applied to produce probabilities.\n",
        "\n",
        "Sure! Here‚Äôs a **standard theory explanation of forward propagation** in neural networks, framed formally like you‚Äôd see in a textbook or lecture notes:\n",
        "\n",
        "___\n",
        "\n",
        "### **4. Summary of Forward Pass**\n",
        "\n",
        "1. Input $X$ ‚Üí Linear transformation ‚Üí Pre-activation $z^{[1]}$\n",
        "2. Apply activation ‚Üí $a^{[1]}$\n",
        "3. Repeat for all hidden layers\n",
        "4. Compute output layer ‚Üí $\\hat{y}$\n",
        "\n",
        "**Vectorized form** ensures efficient computation across the entire batch.\n",
        "\n",
        "\n",
        "### **Key Points**\n",
        "\n",
        "* Forward propagation is **deterministic**: given inputs and weights, outputs are uniquely determined.\n",
        "* It forms the **first step of training**: outputs are compared with true labels to compute the loss.\n",
        "* The computed activations are also used in **backpropagation** to update weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "xpsKa-d0DeY1"
      },
      "outputs": [],
      "source": [
        "def forwardPropogation(X, w1, b1, w2, b2, w3, b3):\n",
        "  \"\"\"\n",
        "  Input -> Hidden1(Relu) -> Hidden2(Relu) --> Output(Linear)\n",
        "\n",
        "  Returns:\n",
        "    Pre-activations and activations for each layer\n",
        "  \"\"\"\n",
        "  z1 = X @ w1 + b1\n",
        "  a1 = relu(z1)\n",
        "\n",
        "  z2 = a1 @ w2 + b2\n",
        "  a2 = relu(z2)\n",
        "\n",
        "  z3 = a2 @ w3 + b3\n",
        "\n",
        "  return z1, a1, z2, a2, z3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0FqodzwJiBK"
      },
      "source": [
        "### **1. What is Backpropagation?**\n",
        "\n",
        "* Backpropagation (short for *backward propagation of errors*) is the algorithm used to **train neural networks**.\n",
        "* It computes how much each weight in the network contributed to the error (loss), and then updates the weights in the right direction using gradient descent.\n",
        "\n",
        "Mathematically, it applies the **chain rule of calculus** to propagate gradients from the output layer backward through the network.\n",
        "\n",
        "---\n",
        "\n",
        "## **2. The Process**\n",
        "\n",
        "### Step 1: Forward Pass\n",
        "\n",
        "* Input flows through the network, producing prediction $\\hat{y}$.\n",
        "* Loss function (e.g., MSE) compares $\\hat{y}$ with true label $y$.\n",
        "\n",
        "$$\n",
        "L = \\frac{1}{N} \\sum_{i=1}^N (\\hat{y}_i - y_i)^2\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### Step 2: Compute Gradient at Output Layer\n",
        "\n",
        "For the output layer pre-activation $z^{[3]}$:\n",
        "\n",
        "$$\n",
        "\\delta^{[3]} = \\frac{\\partial L}{\\partial z^{[3]}}\n",
        "$$\n",
        "\n",
        "* For MSE and linear output:\n",
        "\n",
        "$$\n",
        "\\delta^{[3]} = (\\hat{y} - y)\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### Step 3: Backpropagate to Hidden Layer 2\n",
        "\n",
        "Using chain rule:\n",
        "\n",
        "$$\n",
        "\\delta^{[2]} = (\\delta^{[3]} W^{[3]T}) \\odot g'(z^{[2]})\n",
        "$$\n",
        "\n",
        "* $g'(\\cdot)$ is derivative of activation (for ReLU: $g'(z)=1$ if $z>0$, else $0$).\n",
        "* This tells us how much each hidden neuron contributed to the error.\n",
        "\n",
        "---\n",
        "\n",
        "### Step 4: Backpropagate to Hidden Layer 1\n",
        "\n",
        "$$\n",
        "\\delta^{[1]} = (\\delta^{[2]} W^{[2]T}) \\odot g'(z^{[1]})\n",
        "$$\n",
        "\n",
        "* Same logic, one layer earlier.\n",
        "\n",
        "---\n",
        "\n",
        "### Step 5: Compute Gradients for Weights and Biases\n",
        "\n",
        "Gradients are computed as:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial W^{[l]}} = a^{[l-1]T} \\delta^{[l]}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial b^{[l]}} = \\sum_{i=1}^{N} \\delta^{[l]}_i\n",
        "$$\n",
        "\n",
        "* Where $a^{[l-1]}$ are activations from the previous layer.\n",
        "* These gradients tell how much each weight and bias should change.\n",
        "\n",
        "---\n",
        "\n",
        "### Step 6: Update Parameters\n",
        "\n",
        "Using gradient descent with learning rate $\\alpha$:\n",
        "\n",
        "$$\n",
        "W^{[l]} := W^{[l]} - \\alpha \\frac{\\partial L}{\\partial W^{[l]}}\n",
        "$$\n",
        "\n",
        "$$\n",
        "b^{[l]} := b^{[l]} - \\alpha \\frac{\\partial L}{\\partial b^{[l]}}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "9OmPuUPmFMx_"
      },
      "outputs": [],
      "source": [
        "def backPropogation(X, Y_true, z1, a1, z2, a2, Y_pred, W2, W3):\n",
        "    batch_size = len(X)\n",
        "    # === Output Layer ===\n",
        "\n",
        "    ## Derivative of MSE wrt. predictions => ùõøL/ùõøY_pred\n",
        "    dY_pred = (2/m)*(Y_pred - Y_true)\n",
        "\n",
        "    ## Gradients for W3 and b3\n",
        "    dw3 = a2.T @ dY_pred\n",
        "    db3 = dY_pred.sum(axis = 0, keepdims=True)\n",
        "\n",
        "    # === Hidden Layer 2 ===\n",
        "    da2 = dY_pred @ w3.T\n",
        "    dz2 = da2 * relu_derivative(z2)\n",
        "    dw2 = a1.T @ dz2\n",
        "    db2 = dz2.sum(axis = 0, keepdims = True)\n",
        "\n",
        "    # === Hidden Layer 1 ==\n",
        "    da1 = dY_pred @ w2.T\n",
        "    dz1 = da1*relu_derivative(z1)\n",
        "    dw1 = X.T @ dz1\n",
        "    db1 = dz1.sum(axis = 0, keepdims = True)\n",
        "\n",
        "    return dw1, db1, dw2, db2, dw3, db3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUiT5bnwo6dA"
      },
      "source": [
        "**Training of Our Neural Networks**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "i4YngsGeoZNq"
      },
      "outputs": [],
      "source": [
        "def train_neural_network(deets, X_train, Y_train, X_test, Y_test, epochs=200, patience=10):\n",
        "    # Initializing all the weights:\n",
        "    w1, b1, w2, b2, w3, b3 = xavier_initialization(1, hidden1, hidden2)\n",
        "    hidden1, hidden2, learning_rate = deets\n",
        "    best_test_loss = float('inf')\n",
        "    best_weights = None\n",
        "    patience_counter = 1\n",
        "\n",
        "    train_losses = []\n",
        "    test_losses = []\n",
        "\n",
        "    txt = f\"\"\"\n",
        "    Starting Training...\n",
        "    Architecture: 1 ‚Üí {hidden1} ‚Üí {hidden2} ‚Üí 1\n",
        "    Learning Rate: {learning_rate}\n",
        "    Max Epochs: {epochs}, Early Stopping Patience: {patience}\n",
        "    \"\"\"\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "      # Forward Propogation\n",
        "      z1, a1, z2, a2, Y_pred_train = forwardPropogation(X_train, w1, b1, w2, b2, w3, b3)\n",
        "      # Backward Propogation\n",
        "      train_loss = MSE(Y_train, Y_pred_train)\n",
        "      # Backward pass\n",
        "      dw1, db1, dw2, db2, dw3, db3 = backPropogation(X_train, Y_train, z1,a1,z2, a2, Y_pred_train, w2, w3)\n",
        "      # Gradient Descent update:\n",
        "      w1 -= learning_rate * dw1\n",
        "      b1 -= learning_rate * db1\n",
        "      w2 -= learning_rate * dw2\n",
        "      b2 -= learning_rate * db2\n",
        "      w3 -= learning_rate * dw3\n",
        "      b3 -= learning_rate * db3\n",
        "\n",
        "      # Validation\n",
        "      _,_,_,_,Y_pred_test = forwardPropogation(X_test,w1,b1, w2, b2, w3, b3)\n",
        "      test_loss = MSE(Y_test, Y_pred_test)\n",
        "\n",
        "      # Track Losses\n",
        "      train_losses.append(train_loss)\n",
        "      test_losses.append(test_loss)\n",
        "\n",
        "      # Logging\n",
        "      if(epoch+1)%20 == 0:\n",
        "        print(f\"Epoch {epoch+1:3d}: Train Loss = {train_loss:.6f}, Test Loss = {test_loss:.6f}\")\n",
        "      if test_loss < best_test_loss:\n",
        "        best_test_loss = test_loss\n",
        "        best_weights = (w1.copy(), b1.copy(), w2.copy(), b2.copy(), w3.copy(), b3.copy())\n",
        "        patience_counter = 0\n",
        "      else:\n",
        "        patience_counter += 1\n",
        "      if patience_counter >= patience:\n",
        "        print(f\"Early Stopping triggered at epoch {epoch+1}\")\n",
        "        print(f\"Best Test Loss: {best_test_loss:.6f}\")\n",
        "        break\n",
        "    return best_weights, train_losses, test_losses\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyO3u5hivx2hkRnzfxLH0KSt",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
