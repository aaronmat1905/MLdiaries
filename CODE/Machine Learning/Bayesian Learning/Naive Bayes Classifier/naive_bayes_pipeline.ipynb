{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f27a622",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline \n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix, f1_score\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc24fddf",
   "metadata": {},
   "source": [
    "# **MultiNomial Naive Bayes From Scratch**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39c431b",
   "metadata": {},
   "source": [
    "**Naive Bayes Classifier**\n",
    "\n",
    "The Naive Bayes Classifier is a *supervised ML* algorithm used for classification.\n",
    "\n",
    "It belongs to the family of **probabilistic classifiers**, meaning, it makes predictions based on calculating the probability that a given sample belongs to a particular class.\n",
    "\n",
    "It is used in text classification such as:\n",
    "- Email Spam Filtering\n",
    "- Document Classification\n",
    "- Sentiment analysis\n",
    "\n",
    "The Classifier is named \"NAIVE\" because of a core assumption it makes. (Explained below)\n",
    "\n",
    "___\n",
    "**Bayes Theorem**\n",
    "- The Theorem allows us to \"Flip\" a conditional probability. Instead of asking\n",
    "> What is the probability of words given the class, it asks **What's the probability of the class given the words**?\n",
    "\n",
    "*The Formula*\n",
    "$$\n",
    "P(C|X) = \\frac {P(X|C)\\ P(C)}{P(X)}\n",
    "$$\n",
    "\n",
    "**Meaning of these Terms**:\n",
    "- Posterior Probability: P(C|X)\n",
    "  - This is what we want to find\n",
    "  - **Example**: Given a sentence, \"The study was randomized\", what is the probability it is a `METHODS` document?\n",
    "- **Liklihood:** P(X|C)\n",
    "  - \"If we assume that this class is C, what is the probability of seeing this exact sentence X?\n",
    "  - **Example**: \"If we look only at `METHODS` documents, what is the probability of seeing 'The study was randomized'?\n",
    "- **Prior Probability** P(C):\n",
    "  - \"How common is Class C overall, regardless of any data?\"\n",
    "  - **Example**: What is the percentage of all documents in our training set are `METHODS` documents?\n",
    "- **Evidence**: P(X)\n",
    "  - \"What is the overall probablility of seeing this sentence X in the entire dataset?\n",
    "\n",
    "> For classification, we want to find the class C that has the Highest P(C|X). Since, P(X) is the same for all classes, we can ignore it for comparison. We just need to find the class that mazimizes the numerator:\n",
    "$$\n",
    "  Score(C) \\propto P(X|C) \\cdot P(C)\n",
    "$$\n",
    "\n",
    "**Why Naive?**\n",
    "\n",
    "Calculating the liklihood: P(X|C) is extremely difficult. It means finding the probability of finding the **Exact** words: \"The study was randomized\".\n",
    "\n",
    "To make this possible, Naive Bayes, makes its one, big, \"**naive**\" assumption: **All features(words) are conditionally independent given the class**\n",
    "\n",
    "This means that:\n",
    "1. The word \"study\" appearing has no effect on the probability of the word \"randomized\" appearing\n",
    "2. The order of words does not matter\n",
    "\n",
    "*This is why the text model is often called as a **Bag-of-Words** model. It treats the sentence as just a bag containing words, ignoring all grammar and word-oroder.\n",
    "This assumption simplifies the liklihood calculation from one giant, impossible probability into small, easy ones:\n",
    "\n",
    "$$\n",
    "P(X|C) = P(w_1, w_2, ..., w_n) \\approx P(w_1|C) \\cdot P(w_2|C) \\cdot ... \\cdot P(w_n|C)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1eeeea",
   "metadata": {},
   "source": [
    "**Multinomial Bayes Classifier**\n",
    "\n",
    "What we use in the code:\n",
    "1. **Log-Probabilities**\n",
    "  - Why: Multiplying many small probabilities results in a very very small number, may be approximated to 0, wiping out the calculation\n",
    "  - Soln: The *log function* turns multiplication into addition, which is numerically more stable: `log(ab) = log(a) + log(b)\n",
    "  - Formula:\n",
    "  $$\n",
    "    log(Score(c)) \\propto log(P(C)) + Σ_{i=1}^{n} \\ log(P(w_i|C))\n",
    "  $$\n",
    "\n",
    "  - If a datapoint (like a word appears multiple times)\n",
    "  $$\n",
    "    log(Score(c)) \\propto log(P(C)) + Σ_{i=1}^{n} \\ log(P(w_i|C)) \\cdot count(w_i)\n",
    "  $$\n",
    "\n",
    "2. **Zero-Frequency Problem**\n",
    "  - What if the word \"patient\" appears in a new document, but it never appeared in any `METHODS` documents during training?\n",
    "    - P(\"patient\"|METHODS) = 0\n",
    "    - log(0) = -∞\n",
    "  - This single word would make the entire score for the `METHODS` class negative infinity, disqualifying it even if all other words were a perfect match.\n",
    "  - **Solution**: Laplace(or additive) smoothing: *We add one (or a small value α=1.0 in our code) to every word count. This pretends every word in the vocabulary has appeared at least α times in  every class. This ensures that no probability is ever 0.\n",
    "  - **Formula**:\n",
    "  $$\n",
    "  P(w_i|C) = \\frac {count(w_i, C) + α}{(Total\\ Words\\ in\\ C) + (α \\cdot {Total\\ Vocabulary Size})}\n",
    "  $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c199272f",
   "metadata": {},
   "source": [
    "**The Prediction Process**\n",
    "1. Calculate Priors (from fit): For each class find **P(C)** = \"How common is this class?\". Store **log(P(C))**\n",
    "2. Calculate Likelihoods (from fit): For each class, and for every word in the vocabulary, find **P(w|C)** = \"How common is this word in this class?\" (Using laplace smoothing). Store **log(P(w|C))**.\n",
    "3. Calculate Score (in predict):Start with the prior: $\\text{Score} = \\log(P(C))$.Loop through the words in the new document.For each word, add its weighted likelihood: $\\text{Score} += \\text{count}(\\text{word}) \\cdot \\log(P(\\text{word}|C))$.Make Decision: Compare the final log-scores for all classes (e.g., Score(METHODS), Score(RESULTS)). The class with the highest score wins. This is called the Maximum A Posteriori (MAP) estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659b4cd9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
