{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN3Go8KKpJwnEYaKbuuXNPf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aaronmat1905/MLdiaries/blob/main/DecisionTrees_ID3Algorithm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Desicion Trees\n",
        "## **ID3 Algorithm**\n",
        "The ID3 (Iterative Dichotomiser 3) algorithm is a classic algoritm used to build Decision Trees in Machine Learning.\n",
        "\n",
        "\n",
        "\n",
        "*Introduced by Ross Quinlan in 1986*"
      ],
      "metadata": {
        "id": "BHm_55Sb_KgK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "How does ID3 Work? \\\n",
        "\n",
        "**High Level Overview**\\\n",
        "ID3 builds a decision tree by\n",
        "1. Selecting the **best attribute** to split data at each step\n",
        "2. **Recursively partitioning** the dataset into subsets based on that attribute\n",
        "3. Continuing untill all data points in a subset belong to the same class (**No attributes remain**)"
      ],
      "metadata": {
        "id": "rbO4ZoCu_ol6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Steps**\n",
        "1. Calculate the **Entropy** of the DataSet.\n",
        "2. For each attribute, calculate the **Information Gain** if we split on it\n",
        "3. *Select* the attribute with the **maximum Information Gain** as the decision node\n",
        "4. **Create Child nodes** for each possible value of the chosen attribute\n",
        "5. ***Repeat*** the process recursively for each  subset until:\n",
        "- All Samples in a subset are of the same class\n",
        "- No Attributes remain -> Use Majority class\n",
        "- Subset is empty -> also assign majority class of the parent"
      ],
      "metadata": {
        "id": "IxODjToFAFdP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9g9z0khIzuQq"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Entropy**\n",
        "Entropy is a measure of impurity or uncertainity in a dataset.\n",
        "- If a dataset has all samples from a single class, entropy is 0\n",
        "- If a dataset has an Even mix of classes, entropy is **maximum**, where most uncertainity is prevalent\n",
        "\n",
        "For categorical data, it tell you how diverse the categories are:\\\n",
        " *Are there different varietes of categories? if yes, then there might be maximum uncertainity in predicting a category.*\n",
        "\n",
        "\\\n",
        "\n",
        "It's Formula is given by:\n",
        "\n",
        "$$\n",
        "H(S) = - \\sum_{i=1}^{c} p_i \\log_2 p_i\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "\\\n",
        "\n",
        "**H(S)**: Entropy of Data Set\n",
        "\n",
        "**ð‘ª**: Number of unique categories/clases\n",
        "\n",
        "**ð“…_i**: is the Proportion of Samples belonging to class *(The probability of class i in dataset S)*"
      ],
      "metadata": {
        "id": "fwDNA0aWA3Pi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**For a Dataset, How do you calculate the Entropy?**\n",
        "1. Compute Class Probabilities\n",
        "- Find the probability of each class in the dataset.\n",
        "$$\n",
        "P(X) = {No. of times X occurs}/{No. of datapoints}\n",
        "$$"
      ],
      "metadata": {
        "id": "xeGJNKCADfQm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_entropy_of_dataset(data: np.ndarray) -> float:\n",
        "    \"\"\"\n",
        "    Calculate the entropy of the entire dataset using the target variable (last column).\n",
        "\n",
        "    Args:\n",
        "        data (np.ndarray): Dataset where the last column is the target variable\n",
        "\n",
        "    Returns:\n",
        "        float: Entropy value calculated using the formula:\n",
        "               Entropy = -Î£(p_i * log2(p_i)) where p_i is the probability of class i\n",
        "\n",
        "    Example:\n",
        "        data = np.array([[1, 0, 'yes'],\n",
        "                        [1, 1, 'no'],\n",
        "                        [0, 0, 'yes']])\n",
        "        entropy = get_entropy_of_dataset(data)\n",
        "        # Should return entropy based on target column ['yes', 'no', 'yes']\n",
        "    \"\"\"\n",
        "    # TODO: Implement entropy calculation\n",
        "    # Hint: Use np.unique() to get unique classes and their counts\n",
        "    # Hint: Handle the case when probability is 0 to avoid log2(0)\n",
        "    pass"
      ],
      "metadata": {
        "id": "wGolczvh0s_x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}